#!/usr/bin/env python3
# Based on https://www.reddit.com/r/zfs/comments/s0gxp0/ok_i_made_it_tool_to_show_io_for_individual/

# BSD 2-Clause License
#
# Copyright (c) 2022, Openoid LLC, on behalf of the r/zfs community
# All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions are met:
#
# 1. Redistributions of source code must retain the above copyright notice, this
#    list of conditions and the following disclaimer.
#
# 2. Redistributions in binary form must reproduce the above copyright notice,
#    this list of conditions and the following disclaimer in the documentation
#    and/or other materials provided with the distribution.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

PROGRAM_VERSION = '1.1.0-dev'

import argparse
import os
import re
import sys
import time

class Dataset:
    timestamp = 0
    name = ''
    writes = 0
    nwritten = 0
    reads = 0
    nread = 0

    def __init__(self, data = None):
        if data:
            self.timestamp = int(data['timestamp'])
            self.name = data['dataset_name']
            self.writes = int(data['writes'])
            self.nwritten = int(data['nwritten'])
            self.reads = int(data['reads'])
            self.nread = int(data['nread'])

if sys.platform.startswith("linux"):
    import glob

    def parseDatasets(pool):
        datasets = {}
        for file in glob.glob(os.path.join('/proc/spl/kstat/zfs', glob.escape(pool), 'objset-*')):
            # Shortened example of these files:
            #############################################
            # 31 1 0x01 7 2160 6165792836 1634992995579
            # name             type data
            # dataset_name     7    rpool/ROOT/default
            #############################################
            # Field 7 of the header is a nanosecond data snapshot timestamp.
            # Conveniently, dataset names may not contain spaces.
            with open(file, 'r') as f:
                header, _fieldnames, *fields = [line.split() for line in f]
                fields.append(("timestamp", None, header[6]))
                ds = Dataset({field[0]: field[2] for field in fields if len(field) > 2})
                datasets[ds.name] = ds
        return datasets

elif sys.platform.startswith("freebsd"):
    try:
        # Attempt to use py-sysctl if available for best performance
        import sysctl
    except ImportError:
        # Otherwise make a simple sysctl(8) polyfill hopefully sufficient for our needs
        import subprocess

        class SysctlOid:
            def __init__(self, name = '', value = None):
                self.name = name
                self.value = value

        class sysctl:
            def filter(oid):
                r = subprocess.run(["sysctl", "-e", oid], capture_output=True, check=True, text=True)
                stats = [line.split("=", 2) for line in r.stdout.split("\n")]
                return [SysctlOid(*nv) for nv in stats if len(nv) == 2]

    from collections import defaultdict

    def parseDatasets(pool):
        oid = "kstat.zfs." + pool + ".dataset."
        oidlen = len(oid)
        timestamp = time.monotonic_ns()
        ds = defaultdict(lambda: { 'timestamp': timestamp })
        for ctl in sysctl.filter(oid):
            name = ctl.name[oidlen:].split(".", 2)
            if len(name) == 2:
                objset, oid = name
                ds[objset][oid] = ctl.value

        return {dataset.name: dataset for dataset in map(Dataset, ds.values())}

else:
    print("Unsupported platform: " + sys.platform)
    exit(1)

class DatasetDiff:
    def __init__(self, old, new):
        self.name = new.name
        self.timediff = (new.timestamp - old.timestamp) / 1e9
        self.wps = (new.writes - old.writes) / self.timediff
        self.wMBps = (new.nwritten - old.nwritten) / self.timediff
        self.rps = (new.reads - old.reads) / self.timediff
        self.rMBps = (new.nread - old.nread) / self.timediff

        try:
            self.rareq_sz = (new.nread - old.nread) / (new.reads - old.reads)
        except ZeroDivisionError:
            self.rareq_sz = 0

        try:
            self.wareq_sz = (new.nwritten - old.nwritten) / (new.writes - old.writes)
        except ZeroDivisionError:
            self.wareq_sz = 0

    def nonzero(self):
        """True if this DatasetDiff has any non-zero deltas"""
        return self.wps or self.wMBps or self.rps or self.rMBps

def calcDiff(prevdatasets, datasets):
    return [DatasetDiff(prevdatasets[key], datasets[key]) for key in datasets.keys() & prevdatasets.keys()]

def calcDiffFromStart(datasets):
    zero = Dataset()
    return [DatasetDiff(zero, dataset) for dataset in datasets.values()]

sorts = {
    'name':  {'key': lambda x: x.name},
    'rps':   {'key': lambda x: x.rps,   'reverse': True},
    'wps':   {'key': lambda x: x.wps,   'reverse': True},
    'rMBps': {'key': lambda x: x.rMBps, 'reverse': True},
    'wMBps': {'key': lambda x: x.wMBps, 'reverse': True},
}

# Keep this ordered alphabetically
parser = argparse.ArgumentParser(description='iostat for ZFS datasets', add_help=False)
parser.add_argument('dataset', type=str, nargs='+', help='ZFS dataset')
parser.add_argument('-b', dest='binaryprefix', default=False, action='store_true', help='use binary (power-of-two) prefixes')
parser.add_argument('-c', dest='count', type=int, help='number of reports generated')
parser.add_argument('-h', '--help', action='help', help='show this help message and exit')
parser.add_argument('-i', dest='interval', default=1, type=float, help='interval between reports (in seconds)')
parser.add_argument('-n', dest='nonrecursive', default=False, action='store_true', help='do not recurse into child datasets')
parser.add_argument('-o', dest='overwrite', default=False, action='store_true', help='overwrite old reports in terminal')
group = parser.add_mutually_exclusive_group()
group.add_argument('-P', dest='fullname', default=None, action='store_true', help='display dataset names on a single line')
group.add_argument('-p', dest='fullname', default=None, action='store_false', help='display dataset names as an abbreviated tree')
parser.add_argument('-s', dest='sort', default='name', choices=sorts.keys(), help='sort by the specified field')
parser.add_argument('-y', dest='skipsummary', default=False, action='store_true', help='skip the initial "summary" report')
parser.add_argument('-V', '--version', action='version', version='%(prog)s ' + PROGRAM_VERSION)
parser.add_argument('-z', dest='nonzero', default=False, action='store_true', help='suppress datasets with zero activity')

args = parser.parse_args()

if (args.skipsummary == True) and (args.count):
    args.count = args.count + 1;

prefixmultiplier = 1.0e3
if args.binaryprefix:
    prefixmultiplier = 2.0**10

# Enable full paths if we're not sorting by name or in nonzero mode, unless otherwise specified
if args.fullname is None:
    args.fullname = args.sort != 'name' or args.nonzero

pools = {dataset.split('/')[0] for dataset in args.dataset}
if args.nonrecursive:
    # Make each match exact.
    dataset_pattern = re.compile("|".join(map(lambda ds: "\A" + re.escape(ds) + "\Z", sorted(args.dataset))))
else:
    # Accept either an exact match or one with an additional component
    dataset_pattern = re.compile("|".join(map(lambda ds: "\A" + re.escape(ds) + "(?:\Z|/[^/]+)", sorted(args.dataset))))

try:
    prevdatasets = {}
    index = 0
    while(True):
        datasets = {}
        for pool in pools:
            datasets.update(parseDatasets(pool))

        datasets = {name: dataset for name, dataset in datasets.items() if dataset_pattern.match(name)}

        diff = calcDiff(prevdatasets, datasets)
        if index == 0:
            diff = calcDiffFromStart(datasets)

        if args.nonzero:
            diff = list(filter(lambda d: d.nonzero(), diff))

        # Always sort by name first, so the main sort has it as a secondary
        diff.sort(**sorts['name'])
        if args.sort != 'name':
            diff.sort(**sorts[args.sort])

        if diff and ((args.skipsummary == False) or (index > 0)):
            if (args.overwrite) and (((args.skipsummary == False) and (index > 0)) or (index > 1)):
                for _ in range(linesprinted + 2):
                  print("\033[F", end="")

            linesprinted = 0

            print('{:40s} {:>10s} {:>10s} {:>10s} {:>10s} {:>10s} {:>10s}'
                .format('dataset', 'w/s', 'wMB/s', 'r/s', 'rMB/s', 'wareq-sz', 'rareq-sz'))

            last_path = []
            for d in diff:
                if args.fullname:
                    name = d.name
                else:
                    cur_path = d.name.split('/')
                    # Unmounted intermediate datasets can leave confusing gaps.
                    # Find the longest shared segment with the previous path,
                    # and print any missing intermediates.
                    common = os.path.commonprefix([last_path, cur_path])
                    for i, segment in enumerate(cur_path[len(common):-1]):
                        print(('   ' * (len(common) + i)) + segment)
                    name = ('   ' * (len(cur_path) - 1)) + cur_path[-1]
                    last_path = cur_path

                print('{:40s} {:>10.2f} {:>10.2f} {:>10.2f} {:>10.2f} {:>10.2f} {:>10.2f}'.format(name,
                    d.wps, d.wMBps/prefixmultiplier/prefixmultiplier,
                    d.rps, d.rMBps/prefixmultiplier/prefixmultiplier,
                    d.wareq_sz/prefixmultiplier, d.rareq_sz/prefixmultiplier))
                linesprinted += 1
            print('')

        prevdatasets = datasets
        index = index + 1

        if (args.count) and (index >= args.count):
            break

        time.sleep(args.interval)

except KeyboardInterrupt:
    print('')
